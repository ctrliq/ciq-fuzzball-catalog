values:
  - name: ModelName
    display_name: Name of the HuggingFace model to use.
    #string_value: HuggingFaceTB/SmolLM2-360M-Instruct
    string_value: HuggingFaceTB/SmolLM3-3B
    display_category: Model Configuration
  - name: MaxContextSize
    display_name: Maximum context size. This will impact GPU memory usage.
    uint_value: 8192
    display_category: Model Configuration

  - name: ModelVolume
    display_name: Volume to store the HuggingFace cache directory.
    string_value: volume://user/ephemeral
    display_category: Storage
  - name: HuggingfaceCache
    display_name: Path relative to the root of the storage volume used as the HuggingFace cache.
    string_value: ".cache/huggingface/hub"
    display_category: Storage


  - name: ServerGPUModel
    display_name: GPU model to request
    string_value: NVIDIA L4
    display_category: Resources
  - name: ServerCores
    display_name: Cores to allocate for the LLM server job.
    uint_value: 4
    display_category: Resources
  - name: ServerMemory
    display_name: Amount of memory to allocate for the LLM server job.
    string_value: 12GiB
    display_category: Resources

  - name: "ServiceScope"
    display_name: >
      Who can access this LLM chat service. Defaults to 'user'.
    string_value: user
    options_input:
      options: [user, group, organization]
    display_category: "Access"

  - name: VllmVersion
    display_name: Version of vLLM to use as the API server.
    string_value: v0.13.0
    display_category: Versions
