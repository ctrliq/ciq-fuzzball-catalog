# Copyright 2025 CIQ, Inc. All rights reserved.

{{- $vllm_api_key := (randInt 0 10000000 | printf "%8d" | b64enc ) }}
{{- $vllm_api_port := randInt 10000 32000 }}
{{- $ui_port := randInt 10000 32000 }}

{{- $data_mount := "/data" }}

version: v1
volumes:
  data:
    reference: {{ .ModelVolume }}
defaults:
  job:
    mounts:
      data:
        location: {{ $data_mount }}

services:
  vllm:
    network:
      ports:
        - name: openai-api
          port: {{ $vllm_api_port }}
          protocol: tcp
    image:
      uri: docker://vllm/vllm-openai:{{ .VllmVersion }}
    resource:
      cpu:
        cores: {{ .ServerCores }}
      memory:
        size: {{ .ServerMemory }}
      devices:
        nvidia.com/gpu: 1
      annotations:
        nvidia.com/gpu.model: "{{ .ServerGPUModel }}"
    script: |
      #!/bin/sh
      HF_HUB_CACHE={{ $data_mount }}/{{ .HuggingfaceCache }}
      export HF_HUB_CACHE
      mkdir -p "$HF_HUB_CACHE" || exit 1
      vllm serve --port={{ $vllm_api_port }} --api-key={{ $vllm_api_key }} --max-model-len {{ .MaxContextSize }} {{ .ModelName }}
    readiness-probe:
      http-get:
        path: /v1/models
        port: {{ $vllm_api_port }}
        scheme: http
        http-headers:
          - name: Authorization
            value: "{{ printf "Bearer %s" $vllm_api_key }}"
      initial-delay-seconds: 60
      period-seconds: 30
      failure-threshold: 10
      success-threshold: 1


  chat-interface:
    depends-on:
      - name: vllm
        status: running
    persist: true
    network:
      ports:
        - name: web-port
          port: {{ $ui_port }}
          protocol: tcp
      endpoints:
        - name: web
          type: subdomain
          port-name: web-port
          protocol: http
          scope: {{ .ServiceScope }}
    image:
      uri: docker://ghcr.io/yoziru/nextjs-vllm-ui:latest
    resource:
      cpu:
        cores: 2
      memory:
        size: 4GB
    script: |
      #!/bin/sh
      VLLM_URL=http://vllm.svc:{{ $vllm_api_port }}
      VLLM_API_KEY="{{ $vllm_api_key }}"
      VLLM_TOKEN_LIMIT={{ .MaxContextSize }}
      PORT={{ $ui_port }}
      HOSTNAME="0.0.0.0"
      export VLLM_URL VLLM_API_KEY VLLM_TOKEN_LIMIT PORT HOSTNAME
      cd /opt/app
      node server.js
